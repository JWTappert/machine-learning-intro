# Exercise #7 Handwritten Digit Recognition

For this exercise we were to try and classify a large dataset of handwritten digits. The numbers zero through nine were handwritten by eighty different people and photos of the handwritten digits were taken and a large dataset was composed. Each of the handwritten digits was then stretched to fit into a sixteen by sixteen square and were converted to gray scale with values between zero and two-hundred and fifty six. So in the end we are left with a multi dimensional array of zeros and ones that represents the handwritten digits. In the training dataset these arrays two-hundred and fifty six values were accompanied by an addition ten values that would represent the actual digit for that row. To be clear, the class of the training data was represented as an array of length ten where the values were all zero except for one place where the value was one and that particular index represented the actual handwritten digits for the row.

Starting in Knime I immediately went for a multi layer perceptron learner and predictor. The preprocessing for this data was pretty straight forward. First I separated the features from the classes with a column separator. I then ran the class arrays through the Rule Engine node which allows you to access the columns and run each one through a rule that you write. For each column, the rule was simple: if your value is a 1 then your class is that corresponding integer value from zero to nine. After the class column was appended I slapped the data back together and stripped out the class arrays. Running the training data through the MLP learner with default settings and ten folds cross validation resulted in an eighty-two percent accuracy in predictions. To start my tuning, I started increasing the iterations. After adding five more it was clear that this was not the right setting to tune, the greatest increase in accuracy I got from increasing the max iterations was like a two percent increase. Next I tried messing with the number of hidden layers and this actually hurt my accuracy on predictions. So I left the hidden layers at one, and set the max iterations to one-hundred and five. The next setting is the number of hidden neurons and this is where I found the most improvements. The default value for the number of hidden neurons is ten and incrementing this value steadily increased the accuracy of my MLP model up until about eighty. After eighty the gains were very small and eventually detrimental. 

The money spot that I used for my multi layer perceptron model was ten folds in my x-partitioner node for cross validation, one-hundred and fix set as the max iterations, one hidden layer, and eighty hidden neurons. These settings resulted in an accuracy of eighty-eight percent in predicting the digit in handwritten images of the digits zero through nine.

![Multi-Layer Perceptron in Knime](multilayerPerceptronStats.PNG)